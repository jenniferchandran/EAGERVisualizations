{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import json\n",
    "import pandas as pd\n",
    "import dash\n",
    "import os\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import pydeck as pdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of df: 33307\n"
     ]
    }
   ],
   "source": [
    "#load the data from the sentiment_full_updated.xlsx file and load it into a dataframe\n",
    "\n",
    "series_type = type(pd.Series([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]))\n",
    "\n",
    "dtype_dict = {\n",
    "    'date': str,\n",
    "    'Hiker trail name': str,\n",
    "    'Hiker Journal Link': str,\n",
    "    'Journal Story': str,\n",
    "    'Start location': str,\n",
    "    'Destination': str,\n",
    "    'Today Miles': str,\n",
    "    'Latitude': str,\n",
    "    'Longitude': str,\n",
    "    'State': str,\n",
    "    'Total Shelters': str,\n",
    "    'Occurrence': str,\n",
    "    'year': str,\n",
    "    'month': str,\n",
    "    'label': str,\n",
    "    # pandas serial object for type of emotion score\n",
    "    'Unnamed: 0': str\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.read_excel('../sentiment_full_updated_with_coords.xlsx', sheet_name='Sheet1', dtype=dtype_dict)\n",
    "\n",
    "print(f'length of df: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columsn in df are Index(['Unnamed: 0', 'date', 'Hiker trail name', 'Hiker Journal Link',\n",
      "       'Journal Story', 'Start location', 'Destination', 'Today Miles',\n",
      "       'Latitude', 'Longitude', 'State', 'Total Shelters', 'Occurrence',\n",
      "       'year', 'month', 'label', 'Emotion_scores'],\n",
      "      dtype='object')\n",
      "number of columns in df is 17\n"
     ]
    }
   ],
   "source": [
    "print(f'columsn in df are {df.columns}')\n",
    "print(f'number of columns in df is {len(df.columns)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columsn in df are Index(['date', 'Hiker trail name', 'Hiker Journal Link', 'Journal Story',\n",
      "       'Start location', 'Destination', 'Today Miles', 'Latitude', 'Longitude',\n",
      "       'State', 'Total Shelters', 'Occurrence', 'year', 'month', 'label',\n",
      "       'Emotion_scores'],\n",
      "      dtype='object')\n",
      "number of columns in df is 16\n"
     ]
    }
   ],
   "source": [
    "# clean the data frame by dropping the column called 'Unnamed: 0'\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "print(f'columsn in df are {df.columns}')\n",
    "print(f'number of columns in df is {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                  object\n",
      "Hiker trail name      object\n",
      "Hiker Journal Link    object\n",
      "Journal Story         object\n",
      "Start location        object\n",
      "Destination           object\n",
      "Today Miles           object\n",
      "Latitude              object\n",
      "Longitude             object\n",
      "State                 object\n",
      "Total Shelters        object\n",
      "Occurrence            object\n",
      "year                  object\n",
      "month                 object\n",
      "label                 object\n",
      "Emotion_scores        object\n",
      "dtype: object\n",
      "Blue\n"
     ]
    }
   ],
   "source": [
    "#print the data type of each column\n",
    "print(df.dtypes)\n",
    "print(df.head()[\"Hiker trail name\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column date has data type <class 'str'>\n",
      "1969-12-31 19:00:43.311000\n",
      "\n",
      "column Hiker trail name has data type <class 'str'>\n",
      "Butch and Sundance\n",
      "\n",
      "column Hiker Journal Link has data type <class 'str'>\n",
      "https://www.trailjournals.com//journal/entry/591693\n",
      "\n",
      "column Journal Story has data type <class 'str'>\n",
      "Rained hard most of night, but eased early.  I packed up at 6:30 and was first out of camp.  Tread-way continued to be excellent, although muddy and slippery in many places due to prior evening's rain.  The day could be broken down in thirds.  First, we had tough climbs into and out of Horse Gap and Cooper Gap.  The middle third was pleasant rolling trail with a rest/lunch/water filtration stop at Justus Creek.  The last third was sociable finished off by a brutal climb up to Rampart Mountain (followed by new friends enjoying sunset together). [Pack felt heavy second half of day.] [Checked in with Mighty Blue for podcast.]\n",
      "(Friends: Pritchett, Haillie, Livvy, Scars, El, Vapor (blind), Boss, and section hikers including Josh, Laura & Goose.)\n",
      "\n",
      "BHM\n",
      "How to follow RTK:\n",
      "Blog:  http://www.rtkchallenge.com/bl...\n",
      "YouTube:  https://www.youtube.com/channe...\n",
      "Podcast:  http://returningtokatahdin.lib...\n",
      "Instagram:  https://www.instagram.com/rtk_...\n",
      "Please consider joining the \"RTK AT Challenge\":  https://fundraise.appalachiant...\n",
      "\n",
      "column Start location has data type <class 'float'>\n",
      "Daleville, Va\n",
      "\n",
      "\n",
      "column Destination has data type <class 'str'>\n",
      "low gap shelter\n",
      "\n",
      "column Today Miles has data type <class 'str'>\n",
      "19.8\n",
      "\n",
      "column Latitude has data type <class 'float'>\n",
      "nan\n",
      "\n",
      "column Longitude has data type <class 'float'>\n",
      "-83.646\n",
      "\n",
      "column State has data type <class 'float'>\n",
      "nan\n",
      "\n",
      "column Total Shelters has data type <class 'str'>\n",
      "28\n",
      "\n",
      "column Occurrence has data type <class 'str'>\n",
      "2\n",
      "\n",
      "column year has data type <class 'str'>\n",
      "2016\n",
      "\n",
      "column month has data type <class 'str'>\n",
      "5\n",
      "\n",
      "column label has data type <class 'str'>\n",
      "joy\n",
      "\n",
      "column Emotion_scores has data type <class 'str'>\n",
      "[{'label': 'anger', 'score': 0.002348484005779028}, {'label': 'disgust', 'score': 0.026278506964445114}, {'label': 'fear', 'score': 0.009985250420868397}, {'label': 'joy', 'score': 0.0028527716640383005}, {'label': 'neutral', 'score': 0.09081683307886124}, {'label': 'sadness', 'score': 0.8552387952804565}, {'label': 'surprise', 'score': 0.012479421682655811}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean the data frame by dropping the specifiying the data type of each column\n",
    "\n",
    "\n",
    "# print out a random sample of each of the columns alongside its data type\n",
    "for col in df.columns:\n",
    "    print(f'column {col} has data type {type(df[col].values[0])}')\n",
    "    temp_sample = df[col].sample(1)\n",
    "    print(temp_sample.values[0])\n",
    "    print('')\n",
    "\n",
    "# cast all elemnts in Start location to string\n",
    "df['Start location'] = df['Start location'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "date: 1969-12-31 19:00:42.370000\n",
      "\n",
      "type of date is <class 'str'>\n",
      "-------------------------\n",
      "Hiker trail name: Blue\n",
      "\n",
      "type of Hiker trail name is <class 'str'>\n",
      "-------------------------\n",
      "Hiker Journal Link: https://www.trailjournals.com//journal/entry/518909\n",
      "\n",
      "type of Hiker Journal Link is <class 'str'>\n",
      "-------------------------\n",
      "Journal Story: I've been saving my first post for years.I had contemplated a thru-hike in 2010, belatedly realizing after much planning and emotional investment that the timing wasn't right. I was left feeling very bummed, thinking I had somehow missed my window. The shakedown hikes I had done convinced me that my dream was real, that I indeed could see myself on the trail for six months....thus making it all the more worse when I called off the hunt.\n",
      "In 2014, my wife and I took the family to Patagonia. In preparing for that trip, my thru-hiking spark was relit and I quickly realized that 2016, devoid of significant family events, was the perfect year to tackle the AT.\n",
      "Throughout 2015, I planned. Hunted for the right gear, took the time to get in good shape, figured out a Flip Flop suited me well, and watched the clock on the wall tick all too slowly.\n",
      "So, finally, 2016 is here, and I can't wait!   On April 24th, I walk into the Len Foote Hike Inn. On April 25th, I'll stand atop Springer, ready for an adventure years in the making.\n",
      "\n",
      "type of Journal Story is <class 'str'>\n",
      "-------------------------\n",
      "Start location: nan\n",
      "\n",
      "type of Start location is <class 'str'>\n",
      "-------------------------\n",
      "Destination: finally2016\n",
      "\n",
      "type of Destination is <class 'str'>\n",
      "-------------------------\n",
      "Today Miles: 0\n",
      "\n",
      "type of Today Miles is <class 'str'>\n",
      "-------------------------\n",
      "Latitude: nan\n",
      "\n",
      "type of Latitude is <class 'float'>\n",
      "-------------------------\n",
      "Longitude: nan\n",
      "\n",
      "type of Longitude is <class 'float'>\n",
      "-------------------------\n",
      "State: nan\n",
      "\n",
      "type of State is <class 'float'>\n",
      "-------------------------\n",
      "Total Shelters: 42\n",
      "\n",
      "type of Total Shelters is <class 'str'>\n",
      "-------------------------\n",
      "Occurrence: 1\n",
      "\n",
      "type of Occurrence is <class 'str'>\n",
      "-------------------------\n",
      "year: 2016\n",
      "\n",
      "type of year is <class 'str'>\n",
      "-------------------------\n",
      "month: 1\n",
      "\n",
      "type of month is <class 'str'>\n",
      "-------------------------\n",
      "label: sadness\n",
      "\n",
      "type of label is <class 'str'>\n",
      "-------------------------\n",
      "Emotion_scores: [{'label': 'anger', 'score': 0.0031451925169676542}, {'label': 'disgust', 'score': 0.0011312731076031923}, {'label': 'fear', 'score': 0.0020018790382891893}, {'label': 'joy', 'score': 0.004146144725382328}, {'label': 'neutral', 'score': 0.006345564033836126}, {'label': 'sadness', 'score': 0.9733763337135315}, {'label': 'surprise', 'score': 0.009853655472397804}]\n",
      "\n",
      "type of Emotion_scores is <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# grab a row called test_row_with_nan_coords from this link https://www.trailjournals.com//journal/entry/518909\n",
    "sample = df[df['Hiker Journal Link'] == 'https://www.trailjournals.com//journal/entry/518909']\n",
    "\n",
    "# print out the sample with the format column names: values\n",
    "for column in sample.columns:\n",
    "    print(\"-------------------------\")\n",
    "    print(column + ': ' + str(sample[column].values[0]))\n",
    "    print(\"\")\n",
    "    print(f'type of {column} is {type(sample[column].values[0])}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nan values in Emotion_scores is 7558\n",
      "number of non-nan values in Emotion_scores is 25749\n",
      "total number of values in Emotion_scores is 33307\n"
     ]
    }
   ],
   "source": [
    "# print(df.head()['Emotion_scores'].values[0][3])\n",
    "# print(df.head()['Emotion_scores'].values[0])\n",
    "# new_emo = eval(df.head()['Emotion_scores'].values[0])\n",
    "# print(type(new_emo))\n",
    "\n",
    "emotion_nan_counter = 0\n",
    "# loop over all the values in the Emotion_scores column and convert them to dictionaries\n",
    "for i in range(len(df['Emotion_scores'].values)):\n",
    "    super_value = df['Emotion_scores'].values[i]\n",
    "    # print(f'type of super_value is {type(super_value)}')\n",
    "    # print(f'super_value is {super_value}')\n",
    "    if type(super_value) != type(\"string\"):\n",
    "        emotion_nan_counter += 1\n",
    "        continue\n",
    "    value = super_value[0]\n",
    "    # print(value)\n",
    "    # print(f'type of value is {type(value)}')\n",
    "\n",
    "print(f'number of nan values in Emotion_scores is {emotion_nan_counter}')\n",
    "print(f'number of non-nan values in Emotion_scores is {len(df[\"Emotion_scores\"].values) - emotion_nan_counter}')\n",
    "print(f'total number of values in Emotion_scores is {len(df[\"Emotion_scores\"].values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in df is 25749\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# drop any rows that have do not have objects of type string in the Emotion_scores column in their df['Emotion_scores'].values[i]\n",
    "df = df[df['Emotion_scores'].apply(lambda x: type(x) == type(\"string\"))]\n",
    "\n",
    "print(f'number of rows in df is {len(df)}')\n",
    "# using eval to convert the string to a dictionary do that on the entire colum of Emotion_scores\n",
    "\n",
    "df['Emotion_scores'] = df['Emotion_scores'].apply(lambda x: eval(x))\n",
    "\n",
    "\n",
    "\n",
    "# print the type of the first element in the Emotion_scores column\n",
    "print(type(df['Emotion_scores'].values[0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0031451925169676542\n"
     ]
    }
   ],
   "source": [
    "new_thing = df.head(1)['Emotion_scores'][0][0]['score']\n",
    "print(type(new_thing))\n",
    "print(new_thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "<class 'float'>\n",
      "34.65569448\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "sample_new_lat = sample['Latitude'].values[0]\n",
    "print(sample_new_lat)\n",
    "print(type(sample_new_lat))\n",
    "\n",
    "sample = df[df['Hiker Journal Link'] == 'https://www.trailjournals.com//journal/entry/519033']\n",
    "sample_new_lat = sample['Latitude'].values[0]\n",
    "print(sample_new_lat)\n",
    "print(type(sample_new_lat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of float values in Latitude is 13561\n",
      "number of non-float values in Latitude is 12188\n",
      "total number of values in Latitude is 25749\n"
     ]
    }
   ],
   "source": [
    "# print the values of the latitude column in the df if the type of the value is not a float\n",
    "count = 0\n",
    "for i in range(len(df['Latitude'].values)):\n",
    "    value = df['Latitude'].values[i]\n",
    "    if type(value) != type('string'):\n",
    "        count += 1\n",
    "        # print(value)\n",
    "        # print(type(value))\n",
    "print(f'number of float values in Latitude is {count}')\n",
    "print(f'number of non-float values in Latitude is {len(df[\"Latitude\"].values) - count}')\n",
    "print(f'total number of values in Latitude is {len(df[\"Latitude\"].values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nan values in Latitude is 13561\n",
      "not nan values in Latitude is 0\n",
      "len of invalid_coords is 13561\n"
     ]
    }
   ],
   "source": [
    "# loop over every row in the df and print the value of the lat and long columns\n",
    "import math\n",
    "\n",
    "woah_count = 0\n",
    "not_woah_count = 0\n",
    "invalid_coords = []\n",
    "for i in range(len(df['Latitude'].values)):\n",
    "    lat = df['Latitude'].values[i]\n",
    "    long = df['Longitude'].values[i]\n",
    "    if type(lat) != type('string'):\n",
    "        if math.isnan(lat) or math.isnan(long):\n",
    "            woah_count += 1\n",
    "            \n",
    "            invalid_coords.append(i)\n",
    "            continue\n",
    "            # print(\"woah\")\n",
    "        not_woah_count += 1\n",
    "    # print(f'lat is {lat} and long is {long}')\n",
    "    # print(f'type of lat is {type(lat)} and type of long is {type(long)}')\n",
    "    # print('')\n",
    "\n",
    "print(f'number of nan values in Latitude is {woah_count}')\n",
    "print(f'not nan values in Latitude is {not_woah_count}')\n",
    "print(f'len of invalid_coords is {len(invalid_coords)}')\n",
    "invalid_coords.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "\n",
    "exclude_words = [\n",
    "    \"shelter\",\n",
    "    \"shelters\",\n",
    "    \"SHELTER\",\n",
    "    \"sheltered\",\n",
    "    \"Shelter\",\n",
    "    \"Shelters\",\n",
    "    \"SHELTERED\",\n",
    "    \"SHELTERS\",\n",
    "]\n",
    "\n",
    "def custom_scorer(s1, s2):\n",
    "    s1_tokens = [token for token in s1.split() if token not in exclude_words]\n",
    "    s2_tokens = [token for token in s2.split() if token not in exclude_words]\n",
    "    return fuzz.token_set_ratio(s1_tokens, s2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of weather_data is <class 'dict'>\n",
      "len of weather_data is 39025\n"
     ]
    }
   ],
   "source": [
    "# open the weather data file and load it into a dictionary\n",
    "with open('../weather_data.json') as json_file:\n",
    "    weather_data = json.load(json_file)\n",
    "print(f'type of weather_data is {type(weather_data)}')\n",
    "print(f'len of weather_data is {len(weather_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount found in weather data is 2254\n",
      "searched every invalid coord: True\n",
      "difference between invalid_coords and found_in_weather_data is 0\n"
     ]
    }
   ],
   "source": [
    "found_in_weather_data = 0\n",
    "bad_weather_data = 0\n",
    "not_found = 0\n",
    "# get first key in weather_data\n",
    "# first_key = list(weather_data.keys())[0]\n",
    "# print(f'first key in weather_data is {first_key}')\n",
    "# weather_data_dict = weather_data[first_key]\n",
    "# print(f'weather_data_dict is {weather_data_dict}')\n",
    "# print(f'type of weather_data_dict is {type(weather_data_dict)}')\n",
    "# print(f'leys in weather_data_dict are {weather_data_dict.keys()}')\n",
    "\n",
    "for idx in invalid_coords:\n",
    "    # grab 3 values from the df, the journal link, the latitude, and the longitude\n",
    "    journal_link = df['Hiker Journal Link'].values[idx]\n",
    "    lat = df['Latitude'].values[idx]\n",
    "    long = df['Longitude'].values[idx]\n",
    "    if journal_link in weather_data:\n",
    "        curr_dict = weather_data[journal_link]\n",
    "        curr_dict_keys = curr_dict.keys()\n",
    "        if 'cod' in curr_dict_keys:\n",
    "            bad_weather_data += 1\n",
    "        else:\n",
    "            # replace the \"Latitude\" and \"Longitude\" in the df with the values in the curr dict from keys with \"lat\" and \"lon\" keys\n",
    "            df['Latitude'].values[idx] = str(curr_dict['lat'])\n",
    "            df['Longitude'].values[idx] = str(curr_dict['lon'])\n",
    "            # print(f'found new lat {curr_dict[\"lat\"]} in weather data')\n",
    "            # print(f'type of \"Latitude\" is {type(df[\"Latitude\"].values[idx])}')\n",
    "            found_in_weather_data += 1\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "\n",
    "print(f'amount found in weather data is {found_in_weather_data}')\n",
    "print(f'searched every invalid coord: {found_in_weather_data + bad_weather_data + not_found == len(invalid_coords)}')\n",
    "print(f'difference between invalid_coords and found_in_weather_data is {len(invalid_coords) - (found_in_weather_data + bad_weather_data + not_found)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_before_drop is 25749\n",
      "len_after_drop is 14442\n",
      "difference between len_before_drop and len_after_drop is 11307\n"
     ]
    }
   ],
   "source": [
    "# drop all the rows that have values in Latitude that are not type string\n",
    "len_before_drop = len(df)\n",
    "df = df[df['Latitude'].apply(lambda x: type(x) == type(\"string\"))]\n",
    "len_after_drop = len(df)\n",
    "print(f'len_before_drop is {len_before_drop}')\n",
    "print(f'len_after_drop is {len_after_drop}')\n",
    "print(f'difference between len_before_drop and len_after_drop is {len_before_drop - len_after_drop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount added to the df is 2254\n",
      "amount not found in weather data is 11307\n"
     ]
    }
   ],
   "source": [
    "print(f'amount added to the df is {found_in_weather_data}')\n",
    "print(f'amount not found in weather data is {woah_count - found_in_weather_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in year column are ['2016' '2017' '2018' '2019' \"701048'\" '2020' '2021' '2022' '2023' nan]\n",
      "len_before_drop_year is 14442\n",
      "len_after_drop_year is 14437\n",
      "difference between len_before_drop_year and len_after_drop_year is 5\n",
      "amount of bad year values is 1\n",
      "unique values in year column after drop are ['2016' '2017' '2018' '2019' '2020' '2021' '2022' '2023']\n"
     ]
    }
   ],
   "source": [
    "# print the unique values in the year column\n",
    "print(f'unique values in year column are {df[\"year\"].unique()}')\n",
    "len_before_drop_year = len(df)\n",
    "df = df[df['year'].apply(lambda x: type(x) == type(\"string\"))]\n",
    "len_after_drop_year = len(df)\n",
    "df = df[df['year'].apply(lambda x: len(x) == len(\"2016\"))]\n",
    "len_after_drop_bad_year = len(df)\n",
    "print(f'len_before_drop_year is {len_before_drop_year}')\n",
    "print(f'len_after_drop_year is {len_after_drop_year}')\n",
    "print(f'difference between len_before_drop_year and len_after_drop_year is {len_before_drop_year - len_after_drop_year}')\n",
    "print(f'amount of bad year values is {len_after_drop_year - len_after_drop_bad_year}')\n",
    "print(f'unique values in year column after drop are {df[\"year\"].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in month column ['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12']\n"
     ]
    }
   ],
   "source": [
    "print(f'unique values in month column {df[\"month\"].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper methods\n",
    "def count_entries_within_radius(\n",
    "    target_lon, target_lat, radius_miles, currYear, currMonth\n",
    "):\n",
    "    # Convert miles to degrees (roughly, considering Earth's radius)\n",
    "    degrees_per_mile = 1 / 69  # Approximately\n",
    "    radius_deg = radius_miles * degrees_per_mile\n",
    "\n",
    "    # Calculate latitude and longitude bounds for the square around the target point\n",
    "    min_lon = target_lon - radius_deg\n",
    "    max_lon = target_lon + radius_deg\n",
    "    min_lat = target_lat - radius_deg\n",
    "    max_lat = target_lat + radius_deg\n",
    "\n",
    "    # Filter the DataFrame to get entries within the specified latitude and longitude bounds\n",
    "    filtered_df = df[\n",
    "        (df[\"lon\"] >= min_lon)\n",
    "        & (df[\"lon\"] <= max_lon)\n",
    "        & (df[\"lat\"] >= min_lat)\n",
    "        & (df[\"lat\"] <= max_lat)\n",
    "        & (df[\"year\"] == currYear)\n",
    "        & (df[\"month\"] == currMonth)\n",
    "    ]\n",
    "\n",
    "    # Get the count of entries within the radius\n",
    "    count_within_radius = len(filtered_df)\n",
    "\n",
    "    return count_within_radius\n",
    "\n",
    "\n",
    "def monthToNum(shortMonth):\n",
    "    return {\n",
    "        \"Jan\": 1,\n",
    "        \"Feb\": 2,\n",
    "        \"Mar\": 3,\n",
    "        \"Apr\": 4,\n",
    "        \"May\": 5,\n",
    "        \"Jun\": 6,\n",
    "        \"Jul\": 7,\n",
    "        \"Aug\": 8,\n",
    "        \"Sep\": 9,\n",
    "        \"Oct\": 10,\n",
    "        \"Nov\": 11,\n",
    "        \"Dec\": 12,\n",
    "    }[shortMonth]\n",
    "\n",
    "\n",
    "def numTofullMonthName(num):\n",
    "    return {\n",
    "        1: \"January\",\n",
    "        2: \"Febuary\",\n",
    "        3: \"March\",\n",
    "        4: \"April\",\n",
    "        5: \"May\",\n",
    "        6: \"June\",\n",
    "        7: \"July\",\n",
    "        8: \"Augest\",\n",
    "        9: \"September\",\n",
    "        10: \"October\",\n",
    "        11: \"November\",\n",
    "        12: \"December\",\n",
    "    }[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the year column to type int\n",
    "df['year'] = df['year'].astype(int)\n",
    "# cast the month column to type int\n",
    "df['month'] = df['month'].astype(int)\n",
    "\n",
    "# cast the lat and long columns to type float\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2016    3024\n",
      "2017    2730\n",
      "2019    2244\n",
      "2022    2210\n",
      "2021    1619\n",
      "2018    1612\n",
      "2020     531\n",
      "2023     466\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print the count of the number of entries in the df per year\n",
    "print(df['year'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scatter_geo() got an unexpected keyword argument 'centXer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb Cell 24\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# use plotly and the us-terrain map box to plot all of the data just on a graph so we can see where all the data points are\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Plotting the map\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# filter df for the year 2017\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m filtered_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m2017\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39;49mscatter_geo(filtered_df, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                      lat\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLatitude\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                      lon\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLongitude\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                      scope\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39musa\u001b[39;49m\u001b[39m'\u001b[39;49m,  \u001b[39m# Set the map scope to USA\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                      title\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPoints on Map of USA\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                      centXer\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(lat\u001b[39m=\u001b[39;49m\u001b[39m41.90722\u001b[39;49m, lon\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m70.0369\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# show the fig\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justice/Library/CloudStorage/OneDrive-VirginiaTech/Documents/School/EAGERVisualizations/justiceWeather/visualizations/SentimentVsDensity.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m fig\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: scatter_geo() got an unexpected keyword argument 'centXer'"
     ]
    }
   ],
   "source": [
    "# use plotly and the us-terrain map box to plot all of the data just on a graph so we can see where all the data points are\n",
    "# Plotting the map\n",
    "\n",
    "# filter df for the year 2017\n",
    "filtered_df = df[df['year'] == 2017]\n",
    "fig = px.scatter_geo(filtered_df, \n",
    "                     lat='Latitude', \n",
    "                     lon='Longitude',\n",
    "                     scope='usa',  # Set the map scope to USA\n",
    "                     title='Points on Map of USA',\n",
    "                     centXer=dict(lat=41.90722, lon=-70.0369))\n",
    "\n",
    "\n",
    "# show the fig\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the emotion scores for the first row in the df\n",
    "emotion_scores = df['Emotion_scores'].values[0]\n",
    "# pretty prtnt the emotion scores\n",
    "print(json.dumps(emotion_scores, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the columns of the df\n",
    "print(df.columns)\n",
    "\n",
    "# print the unique values in the label column\n",
    "print(df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelToInt(label):\n",
    "    return {\n",
    "        \"surprise\": 2,\n",
    "        \"joy\": 1,\n",
    "        \"neutral\": 0,\n",
    "        \"sadness\": -1,\n",
    "        \"anger\": -2,\n",
    "        \"disgust\": -3,\n",
    "        \"fear\": -4,\n",
    "\n",
    "    }[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to the df called label_int that is the label column but with the values converted to ints\n",
    "df['label_int'] = df['label'].apply(lambda x: labelToInt(x))\n",
    "\n",
    "# print out the counts of the label_int column\n",
    "print(df['label_int'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2=px.density_mapbox(\n",
    "        df,\n",
    "        lat=\"Latitude\",\n",
    "        lon=\"Longitude\",\n",
    "        z=\"label_int\",\n",
    "        radius=2,  # Adjust the radius as needed\n",
    "        center=dict(lat=41.90722, lon=-70.0369),  # Center of the United States\n",
    "        zoom=4,  # Adjust the zoom level as needed\n",
    "        mapbox_style=\"stamen-terrain\",  # You can choose a different map style\n",
    "        title=f\"Temperature Heatmap for everything\",\n",
    "        color_continuous_scale=\"Inferno\",  # Adjust the color scale\n",
    "        range_color=[-4, 2],  # Set the color scale range\n",
    "        hover_data={\n",
    "            \"Latitude\": False,\n",
    "            \"Longitude\": False,\n",
    "            \"label_int\": True,\n",
    "        },  # Include temperature in hover data\n",
    "    )\n",
    "\n",
    "fig_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = df.groupby(['Latitude', 'Longitude'], as_index=False)['label_int'].mean()\n",
    "\n",
    "# drop all the rows that have label_int == 0\n",
    "avg_df = avg_df[avg_df['label_int'] != 0]\n",
    "\n",
    "# Plotting the density map\n",
    "fig_3 = px.density_mapbox(\n",
    "    avg_df,  # Use the grouped DataFrame\n",
    "    lat=\"Latitude\",\n",
    "    lon=\"Longitude\",\n",
    "    z=\"label_int\",\n",
    "    radius=6,\n",
    "    center=dict(lat=avg_df['Latitude'].mean(), lon=avg_df['Longitude'].mean()),\n",
    "    zoom=4,\n",
    "    mapbox_style=\"stamen-terrain\",\n",
    "    title=\"Average Emotion on the Trail\",\n",
    "    color_continuous_scale=\"Inferno\",\n",
    "    range_color=[avg_df['label_int'].min(), avg_df['label_int'].max()],\n",
    "    hover_data={\n",
    "        \"Latitude\": False,\n",
    "        \"Longitude\": False,\n",
    "        \"label_int\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig_3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
